{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Assignment 1"
      ],
      "metadata": {
        "id": "hNXYAuUK0vM8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j-ZGIgIM0ar6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensors with numpy and pytorch"
      ],
      "metadata": {
        "id": "cIcMc_IT0yR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NumPy Tensors :\")\n",
        "# 1D NumPy\n",
        "np_1d = np.array([1, 2, 3, 4])\n",
        "print(\"NumPy 1D Array:\", np_1d)\n",
        "print(\"Shape:\", np_1d.shape)\n",
        "\n",
        "# 2D NumPy\n",
        "np_2d = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "print(\"\\nNumPy 2D Array:\", np_2d)\n",
        "print(\"Shape:\", np_2d.shape)\n",
        "\n",
        "# 3D NumPy\n",
        "numpy_3d = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
        "print(\"\\nNumPy 3D Array:\", numpy_3d)\n",
        "print(\"Shape:\", numpy_3d.shape)\n",
        "\n",
        "print(\"\\nPytorch Tensors:\")\n",
        "# 1D PyTorch\n",
        "torch_1d = torch.tensor([1, 2, 3, 4])\n",
        "print(\"PyTorch 1D Tensor:\", torch_1d)\n",
        "print(\"Shape:\", torch_1d.shape)\n",
        "\n",
        "# 2D PyTorch\n",
        "torch_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(\"\\nPyTorch 2D Tensor:\", torch_2d)\n",
        "print(\"Shape:\", torch_2d.shape)\n",
        "\n",
        "# 3D PyTorch\n",
        "torch_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
        "print(\"\\nPyTorch 3D Tensor:\", torch_3d)\n",
        "print(\"Shape:\", torch_3d.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnAKJ7dK03tc",
        "outputId": "12778f76-ee73-434e-d494-9a32efe9e9bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy Tensors :\n",
            "NumPy 1D Array: [1 2 3 4]\n",
            "Shape: (4,)\n",
            "\n",
            "NumPy 2D Array: [[1 2 3]\n",
            " [4 5 6]]\n",
            "Shape: (2, 3)\n",
            "\n",
            "NumPy 3D Array: [[[1 2]\n",
            "  [3 4]]\n",
            "\n",
            " [[5 6]\n",
            "  [7 8]]]\n",
            "Shape: (2, 2, 2)\n",
            "\n",
            "Pytorch Tensors:\n",
            "PyTorch 1D Tensor: tensor([1, 2, 3, 4])\n",
            "Shape: torch.Size([4])\n",
            "\n",
            "PyTorch 2D Tensor: tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "Shape: torch.Size([2, 3])\n",
            "\n",
            "PyTorch 3D Tensor: tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n",
            "Shape: torch.Size([2, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Operations : Element-wise addition, subtraction, multiplication and division"
      ],
      "metadata": {
        "id": "oUT9kR6u2MF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NumPy Element-wise Operations\")\n",
        "# 1D NumPy Operations\n",
        "a = np.array([1, 2, 3, 4])\n",
        "b = np.array([5, 6, 7, 8])\n",
        "print(\"\\nNumPy 1D Arrays:a=\", a,\"\\nb=\", b)\n",
        "print(\"Addition (a + b):\", a + b)\n",
        "print(\"Subtraction (a - b):\", a - b)\n",
        "print(\"Multiplication (a * b):\", a * b)\n",
        "print(\"Division (a / b):\", a / b)\n",
        "\n",
        "# 2D NumPy Operations\n",
        "c = np.array([[1, 2], [3, 4]])\n",
        "d = np.array([[5, 6], [7, 8]])\n",
        "print(\"\\nNumPy 2D Arrays:\\n a=\", c, \"\\nb=\", d)\n",
        "print(\"Addition (c + d):\\n\", c + d)\n",
        "print(\"Subtraction (c - d):\\n\", c - d)\n",
        "print(\"Multiplication (c * d):\\n\", c * d)\n",
        "print(\"Division (c / d):\\n\", c / d)\n",
        "\n",
        "print(\"\\n--- PyTorch Element-wise Operations ---\")\n",
        "# 1D PyTorch Operations\n",
        "x = torch.tensor([1, 2, 3, 4])\n",
        "y = torch.tensor([5, 6, 7, 8])\n",
        "print(\"\\nPyTorch 1D Tensors:x=\", x,\"\\ny=\", y)\n",
        "print(\"Addition (x + y):\", x + y)\n",
        "print(\"Subtraction (x - y):\", x - y)\n",
        "print(\"Multiplication (x * y):\", x * y)\n",
        "print(\"Division (x / y):\", x / y)\n",
        "\n",
        "# 2D PyTorch Operations\n",
        "p = torch.tensor([[1, 2], [3, 4]])\n",
        "q = torch.tensor([[5, 6], [7, 8]])\n",
        "print(\"\\nPyTorch 2D Tensors:\\np=\", p, \"\\nq=\", q)\n",
        "print(\"Addition (p + q):\\n\", p + q)\n",
        "print(\"Subtraction (p - q):\\n\", p - q)\n",
        "print(\"Multiplication (p * q):\\n\", p * q)\n",
        "print(\"Division (p / q):\\n\", p / q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8LsMv952bh4",
        "outputId": "b5b4797d-296d-4f16-8244-772f422283b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy Element-wise Operations\n",
            "\n",
            "NumPy 1D Arrays:a= [1 2 3 4] \n",
            "b= [5 6 7 8]\n",
            "Addition (a + b): [ 6  8 10 12]\n",
            "Subtraction (a - b): [-4 -4 -4 -4]\n",
            "Multiplication (a * b): [ 5 12 21 32]\n",
            "Division (a / b): [0.2        0.33333333 0.42857143 0.5       ]\n",
            "\n",
            "NumPy 2D Arrays:\n",
            " a= [[1 2]\n",
            " [3 4]] \n",
            "b= [[5 6]\n",
            " [7 8]]\n",
            "Addition (c + d):\n",
            " [[ 6  8]\n",
            " [10 12]]\n",
            "Subtraction (c - d):\n",
            " [[-4 -4]\n",
            " [-4 -4]]\n",
            "Multiplication (c * d):\n",
            " [[ 5 12]\n",
            " [21 32]]\n",
            "Division (c / d):\n",
            " [[0.2        0.33333333]\n",
            " [0.42857143 0.5       ]]\n",
            "\n",
            "--- PyTorch Element-wise Operations ---\n",
            "\n",
            "PyTorch 1D Tensors:x= tensor([1, 2, 3, 4]) \n",
            "y= tensor([5, 6, 7, 8])\n",
            "Addition (x + y): tensor([ 6,  8, 10, 12])\n",
            "Subtraction (x - y): tensor([-4, -4, -4, -4])\n",
            "Multiplication (x * y): tensor([ 5, 12, 21, 32])\n",
            "Division (x / y): tensor([0.2000, 0.3333, 0.4286, 0.5000])\n",
            "\n",
            "PyTorch 2D Tensors:\n",
            "p= tensor([[1, 2],\n",
            "        [3, 4]]) \n",
            "q= tensor([[5, 6],\n",
            "        [7, 8]])\n",
            "Addition (p + q):\n",
            " tensor([[ 6,  8],\n",
            "        [10, 12]])\n",
            "Subtraction (p - q):\n",
            " tensor([[-4, -4],\n",
            "        [-4, -4]])\n",
            "Multiplication (p * q):\n",
            " tensor([[ 5, 12],\n",
            "        [21, 32]])\n",
            "Division (p / q):\n",
            " tensor([[0.2000, 0.3333],\n",
            "        [0.4286, 0.5000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dot product and matrix multiplication"
      ],
      "metadata": {
        "id": "VTRO0ybA3VOA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c906cd19",
        "outputId": "48940441-1788-451c-9937-db89fd9eb09c"
      },
      "source": [
        "a=np.array([[1,2],[4,5]])\n",
        "b=np.array([[2,4],[3,9]])\n",
        "print(\"Matrix 1:\\n\",a,\"\\nMatrix 2:\\n\",b)\n",
        "matrix_mul=np.matmul(a,b)\n",
        "x=np.array([1,2,3])\n",
        "y=np.array([3,-1,0])\n",
        "print(\"Vector 1:\",x,\"\\nVector 2:\",y)\n",
        "dotProd=np.dot(x,y)\n",
        "\n",
        "print(\"Matrix multiplication of a and b:\\n\",matrix_mul)\n",
        "print(\"Dot product of x and y:\",dotProd)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix 1:\n",
            " [[1 2]\n",
            " [4 5]] \n",
            "Matrix 2:\n",
            " [[2 4]\n",
            " [3 9]]\n",
            "Vector 1: [1 2 3] \n",
            "Vector 2: [ 3 -1  0]\n",
            "Matrix multiplication of a and b:\n",
            " [[ 8 22]\n",
            " [23 61]]\n",
            "Dot product of x and y: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexing and Slicing examples :\n",
        "Boolean masking and extracting subtensor\n",
        "\n"
      ],
      "metadata": {
        "id": "DVV_fDH76xNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NumPy Indexing and Slicing \")\n",
        "# 1D NumPy Array\n",
        "numpy_array_1d = np.array([10, 20, 30, 40, 50])\n",
        "print(\"Original 1D NumPy Array:\", numpy_array_1d)\n",
        "print(\"Element at index 2:\", numpy_array_1d[2])\n",
        "print(\"Slice from index 1 to 3 (exclusive):\", numpy_array_1d[1:4])\n",
        "print(\"Every second element:\", numpy_array_1d[::2])\n",
        "\n",
        "# 2D NumPy Array\n",
        "numpy_array_2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print(\"\\nOriginal 2D NumPy Array:\\n\", numpy_array_2d)\n",
        "print(\"Element at row 1, column 2:\", numpy_array_2d[1, 2])\n",
        "print(\"First row:\", numpy_array_2d[0, :])\n",
        "print(\"Second column:\", numpy_array_2d[:, 1])\n",
        "print(\"Sub-array (first 2 rows, first 2 columns):\\n\", numpy_array_2d[0:2, 0:2])\n",
        "\n",
        "print(\"\\nNumPy Boolean Masking\")\n",
        "# 1D Boolean Masking\n",
        "mask_1d = numpy_array_1d > 25\n",
        "print(\"Boolean mask for 1D array (>25):\", mask_1d)\n",
        "print(\"Elements greater than 25:\", numpy_array_1d[mask_1d])\n",
        "\n",
        "# 2D Boolean Masking\n",
        "mask_2d = numpy_array_2d % 2 == 0 # Even numbers\n",
        "print(\"Boolean mask for 2D array (even numbers):\\n\", mask_2d)\n",
        "print(\"Even numbers in 2D array:\", numpy_array_2d[mask_2d])\n",
        "\n",
        "\n",
        "print(\"\\n--- PyTorch Indexing and Slicing ---\")\n",
        "# 1D PyTorch Tensor\n",
        "torch_tensor_1d = torch.tensor([10, 20, 30, 40, 50])\n",
        "print(\"Original 1D PyTorch Tensor:\", torch_tensor_1d)\n",
        "print(\"Element at index 2:\", torch_tensor_1d[2])\n",
        "print(\"Slice from index 1 to 3 (exclusive):\", torch_tensor_1d[1:4])\n",
        "print(\"Every second element:\", torch_tensor_1d[::2])\n",
        "\n",
        "# 2D PyTorch Tensor\n",
        "torch_tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print(\"\\nOriginal 2D PyTorch Tensor:\\n\", torch_tensor_2d)\n",
        "print(\"Element at row 1, column 2:\", torch_tensor_2d[1, 2])\n",
        "print(\"First row:\", torch_tensor_2d[0, :])\n",
        "print(\"Second column:\", torch_tensor_2d[:, 1])\n",
        "print(\"Sub-tensor (first 2 rows, first 2 columns):\\n\", torch_tensor_2d[0:2, 0:2])\n",
        "\n",
        "print(\"\\nPyTorch Boolean Masking\")\n",
        "# 1D Boolean Masking\n",
        "mask_torch_1d = torch_tensor_1d > 25\n",
        "print(\"Boolean mask for 1D tensor (>25):\", mask_torch_1d)\n",
        "print(\"Elements greater than 25:\", torch_tensor_1d[mask_torch_1d])\n",
        "\n",
        "# 2D Boolean Masking\n",
        "mask_torch_2d = torch_tensor_2d % 2 == 0 # Even numbers\n",
        "print(\"Boolean mask for 2D tensor (even numbers):\\n\", mask_torch_2d)\n",
        "print(\"Even numbers in 2D tensor:\", torch_tensor_2d[mask_torch_2d])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9IfgEzJ7Kb7",
        "outputId": "e39ef46a-f40f-49b7-c7ea-a3deeba40dce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy Indexing and Slicing \n",
            "Original 1D NumPy Array: [10 20 30 40 50]\n",
            "Element at index 2: 30\n",
            "Slice from index 1 to 3 (exclusive): [20 30 40]\n",
            "Every second element: [10 30 50]\n",
            "\n",
            "Original 2D NumPy Array:\n",
            " [[1 2 3]\n",
            " [4 5 6]\n",
            " [7 8 9]]\n",
            "Element at row 1, column 2: 6\n",
            "First row: [1 2 3]\n",
            "Second column: [2 5 8]\n",
            "Sub-array (first 2 rows, first 2 columns):\n",
            " [[1 2]\n",
            " [4 5]]\n",
            "\n",
            "NumPy Boolean Masking\n",
            "Boolean mask for 1D array (>25): [False False  True  True  True]\n",
            "Elements greater than 25: [30 40 50]\n",
            "Boolean mask for 2D array (even numbers):\n",
            " [[False  True False]\n",
            " [ True False  True]\n",
            " [False  True False]]\n",
            "Even numbers in 2D array: [2 4 6 8]\n",
            "\n",
            "--- PyTorch Indexing and Slicing ---\n",
            "Original 1D PyTorch Tensor: tensor([10, 20, 30, 40, 50])\n",
            "Element at index 2: tensor(30)\n",
            "Slice from index 1 to 3 (exclusive): tensor([20, 30, 40])\n",
            "Every second element: tensor([10, 30, 50])\n",
            "\n",
            "Original 2D PyTorch Tensor:\n",
            " tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "Element at row 1, column 2: tensor(6)\n",
            "First row: tensor([1, 2, 3])\n",
            "Second column: tensor([2, 5, 8])\n",
            "Sub-tensor (first 2 rows, first 2 columns):\n",
            " tensor([[1, 2],\n",
            "        [4, 5]])\n",
            "\n",
            "PyTorch Boolean Masking\n",
            "Boolean mask for 1D tensor (>25): tensor([False, False,  True,  True,  True])\n",
            "Elements greater than 25: tensor([30, 40, 50])\n",
            "Boolean mask for 2D tensor (even numbers):\n",
            " tensor([[False,  True, False],\n",
            "        [ True, False,  True],\n",
            "        [False,  True, False]])\n",
            "Even numbers in 2D tensor: tensor([2, 4, 6, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .view(), .reshape(), .unsqueeze(), .squeeze() functions in pytorch"
      ],
      "metadata": {
        "id": "tecazcWW8Kwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PyTorch Tensor Shape Manipulation\")\n",
        "\n",
        "# Original 2D tensor\n",
        "original_tensor = torch.arange(1, 10).reshape(3, 3)\n",
        "print(\"Original Tensor:\\n\", original_tensor)\n",
        "print(\"Original Shape:\", original_tensor.shape)\n",
        "\n",
        "viewed_tensor = original_tensor.view(1, 9) # Reshape to 1 row, 9 columns\n",
        "print(\"\\nTensor after .view(1, 9):\\n\", viewed_tensor)\n",
        "print(\"Shape after .view(1, 9):\", viewed_tensor.shape)\n",
        "\n",
        "viewed_tensor_inferred = original_tensor.view(-1)\n",
        "print(\"Tensor after .view(-1) (flattened):\", viewed_tensor_inferred)\n",
        "print(\"Shape after .view(-1):\", viewed_tensor_inferred.shape)\n",
        "\n",
        "\n",
        "reshaped_tensor = original_tensor.reshape(9, 1)\n",
        "print(\"\\nTensor after .reshape(9, 1):\\n\", reshaped_tensor)\n",
        "print(\"Shape after .reshape(9, 1):\", reshaped_tensor.shape)\n",
        "\n",
        "\n",
        "single_dim_tensor = torch.tensor([1, 2, 3]) # Shape (3,)\n",
        "print(\"\\nOriginal 1D Tensor for unsqueeze:\", single_dim_tensor)\n",
        "print(\"Shape:\", single_dim_tensor.shape)\n",
        "\n",
        "unsqueeze_0 = single_dim_tensor.unsqueeze(0) # Add a dimension at position 0\n",
        "print(\"Tensor after .unsqueeze(0):\", unsqueeze_0)\n",
        "print(\"Shape after .unsqueeze(0):\", unsqueeze_0.shape)\n",
        "\n",
        "unsqueeze_1 = single_dim_tensor.unsqueeze(1) # Add a dimension at position 1\n",
        "print(\"Tensor after .unsqueeze(1):\\n\", unsqueeze_1)\n",
        "print(\"Shape after .unsqueeze(1):\", unsqueeze_1.shape)\n",
        "\n",
        "squeezable_tensor = torch.zeros(1, 10, 1, 5, 1) # Shape (1, 10, 1, 5, 1)\n",
        "print(\"\\nOriginal Squeezable Tensor shape:\", squeezable_tensor.shape)\n",
        "\n",
        "squeezed_tensor = squeezable_tensor.squeeze() # Removes all dimensions of size 1\n",
        "print(\"Tensor after .squeeze():\", squeezed_tensor.shape)\n",
        "\n",
        "squeezed_dim_0 = squeezable_tensor.squeeze(0) # Remove dimension at position 0 only if size is 1\n",
        "print(\"Tensor after .squeeze(0):\", squeezed_dim_0.shape)\n",
        "\n",
        "squeezed_dim_2 = squeezable_tensor.squeeze(2) # Remove dimension at position 2 only if size is 1\n",
        "print(\"Tensor after .squeeze(2):\", squeezed_dim_2.shape)\n",
        "\n",
        "# Squeeze will not remove a dimension if its size is greater than 1\n",
        "no_change_squeeze = torch.zeros(2, 3, 4)\n",
        "print(\"\\nOriginal no_change_squeeze shape:\", no_change_squeeze.shape)\n",
        "print(\"Tensor after .squeeze() on (2,3,4):\", no_change_squeeze.squeeze().shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOwFmGx68b60",
        "outputId": "b74146d6-4ecc-4848-b1e0-922fb575ed08"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Tensor Shape Manipulation\n",
            "Original Tensor:\n",
            " tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "Original Shape: torch.Size([3, 3])\n",
            "\n",
            "Tensor after .view(1, 9):\n",
            " tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
            "Shape after .view(1, 9): torch.Size([1, 9])\n",
            "Tensor after .view(-1) (flattened): tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "Shape after .view(-1): torch.Size([9])\n",
            "\n",
            "Tensor after .reshape(9, 1):\n",
            " tensor([[1],\n",
            "        [2],\n",
            "        [3],\n",
            "        [4],\n",
            "        [5],\n",
            "        [6],\n",
            "        [7],\n",
            "        [8],\n",
            "        [9]])\n",
            "Shape after .reshape(9, 1): torch.Size([9, 1])\n",
            "\n",
            "Original 1D Tensor for unsqueeze: tensor([1, 2, 3])\n",
            "Shape: torch.Size([3])\n",
            "Tensor after .unsqueeze(0): tensor([[1, 2, 3]])\n",
            "Shape after .unsqueeze(0): torch.Size([1, 3])\n",
            "Tensor after .unsqueeze(1):\n",
            " tensor([[1],\n",
            "        [2],\n",
            "        [3]])\n",
            "Shape after .unsqueeze(1): torch.Size([3, 1])\n",
            "\n",
            "Original Squeezable Tensor shape: torch.Size([1, 10, 1, 5, 1])\n",
            "Tensor after .squeeze(): torch.Size([10, 5])\n",
            "Tensor after .squeeze(0): torch.Size([10, 1, 5, 1])\n",
            "Tensor after .squeeze(2): torch.Size([1, 10, 5, 1])\n",
            "\n",
            "Original no_change_squeeze shape: torch.Size([2, 3, 4])\n",
            "Tensor after .squeeze() on (2,3,4): torch.Size([2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison of reshape function in pytorch and numpy"
      ],
      "metadata": {
        "id": "qCXMhS4INjK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Define a large array/tensor size for comparison\n",
        "size = (1000, 1000)\n",
        "\n",
        "# NumPy Reshape\n",
        "numpy_array = np.random.rand(*size)\n",
        "\n",
        "start_time = time.time()\n",
        "reshaped_numpy = numpy_array.reshape(1, 1000000)\n",
        "end_time = time.time()\n",
        "numpy_time = end_time - start_time\n",
        "print(f\"NumPy reshape time: {numpy_time:.6f} seconds\")\n",
        "\n",
        "# PyTorch Reshape\n",
        "torch_tensor = torch.randn(*size)\n",
        "\n",
        "start_time = time.time()\n",
        "reshaped_torch = torch_tensor.reshape(1, 1000000)\n",
        "end_time = time.time()\n",
        "torch_time = end_time - start_time\n",
        "print(f\"PyTorch reshape time: {torch_time:.6f} seconds\")\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "if numpy_time < torch_time:\n",
        "    print(f\"NumPy reshape was faster by {torch_time - numpy_time:.6f} seconds.\")\n",
        "elif torch_time < numpy_time:\n",
        "    print(f\"PyTorch reshape was faster by {numpy_time - torch_time:.6f} seconds.\")\n",
        "else:\n",
        "    print(\"Both reshape operations took approximately the same amount of time.\")"
      ],
      "metadata": {
        "id": "wHinBZw668g8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba29a2c1-9b29-41a6-9225-de69654161fc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy reshape time: 0.000642 seconds\n",
            "PyTorch reshape time: 0.000222 seconds\n",
            "\n",
            "Comparison:\n",
            "PyTorch reshape was faster by 0.000420 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "huwLLkkX6POe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c30e3516"
      },
      "source": [
        "## Broadcasting in PyTorch\n",
        "\n",
        "Broadcasting in PyTorch (and NumPy) allows arithmetic operations between tensors of different shapes, provided that they are compatible. Two dimensions are compatible when:\n",
        "\n",
        "1. They are equal, or\n",
        "2. One of them is 1.\n",
        "\n",
        "When performing an operation, PyTorch stretches the smaller tensor along the singleton dimensions (dimensions of size 1) to match the larger tensor's shape. This avoids the need to explicitly create larger copies of the smaller tensor, which is memory-efficient and speeds up computations. The elements are virtually copied to perform the operation. If dimensions are incompatible, PyTorch will raise a runtime error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8e025da"
      },
      "source": [
        "print(\"PyTorch Broadcasting Example\")\n",
        "\n",
        "# Example 1: Scalar broadcasting\n",
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor(10) # Scalar\n",
        "print(f\"\\nTensor a: {a}, Shape: {a.shape}\")\n",
        "print(f\"Scalar b: {b}, Shape: {b.shape}\")\n",
        "print(f\"a + b (scalar broadcasting): {a + b}\")\n",
        "\n",
        "# Example 2: 1D tensor broadcasting to a 2D tensor\n",
        "c = torch.tensor([[1, 2, 3], [4, 5, 6]]) # Shape (2, 3)\n",
        "d = torch.tensor([10, 20, 30]) # Shape (3,)\n",
        "\n",
        "print(f\"\\nTensor c:\\n{c}, Shape: {c.shape}\")\n",
        "print(f\"Tensor d: {d}, Shape: {d.shape}\")\n",
        "print(f\"c + d (1D to 2D broadcasting):\\n{c + d}\")\n",
        "\n",
        "# Example 3: Different dimensions, compatible shapes\n",
        "e = torch.tensor([[1, 2, 3]]) # Shape (1, 3)\n",
        "f = torch.tensor([[10], [20]]) # Shape (2, 1)\n",
        "\n",
        "print(f\"\\nTensor e:\\n{e}, Shape: {e.shape}\")\n",
        "print(f\"Tensor f:\\n{f}, Shape: {f.shape}\")\n",
        "print(f\"e + f (different dimensions, compatible shapes):\\n{e + f}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "746ddaea"
      },
      "source": [
        "## In-place and Out-of-place Operations in PyTorch\n",
        "\n",
        "In PyTorch, operations can be categorized as either **in-place** or **out-of-place**.\n",
        "\n",
        "### Out-of-place Operations\n",
        "\n",
        "Most PyTorch operations are out-of-place by default. This means they return a *new* tensor with the result of the operation, leaving the original tensor unchanged. For example, `a + b` creates a new tensor to store the sum of `a` and `b`.\n",
        "\n",
        "- **Characteristics:**\n",
        "    - They do not modify the original tensor(s).\n",
        "    - They return a new tensor containing the result.\n",
        "    - They might consume more memory because a new tensor is allocated.\n",
        "    - They are generally safer as they prevent unintended side effects on the original data.\n",
        "\n",
        "### In-place Operations\n",
        "\n",
        "In-place operations modify the tensor they are called on directly, without creating a new tensor. In PyTorch, in-place operations are usually denoted by a trailing underscore (`_`) in their method names (e.g., `add_`, `mul_`, `copy_`).\n",
        "\n",
        "- **Characteristics:**\n",
        "    - They modify the tensor directly, saving memory by not allocating a new tensor.\n",
        "    - They typically return `self` (the modified tensor).\n",
        "    - They can lead to unexpected behavior if not used carefully, especially in computational graphs where gradients are involved (as they break the computational history).\n",
        "    - Examples include `tensor.add_(other_tensor)`, `tensor.mul_(scalar)`, `tensor.zero_()`.\n",
        "\n",
        "### Why use in-place operations?\n",
        "\n",
        "In-place operations are primarily used for:\n",
        "\n",
        "1.  **Memory Efficiency:** When dealing with very large tensors, creating new tensors for every operation can be memory-intensive. In-place operations can help reduce memory footprint.\n",
        "2.  **Performance:** Sometimes, in-place operations can offer a slight performance advantage by avoiding new memory allocations and data copying.\n",
        "\n",
        "### Caution with In-place Operations in Deep Learning\n",
        "\n",
        "When building neural networks with PyTorch, be careful with in-place operations, especially within `torch.autograd` (PyTorch's automatic differentiation engine). In-place operations can cause issues if the original value of the tensor is needed for gradient computation but has been overwritten. PyTorch's autograd system typically expects operations to be out-of-place to correctly track the computational graph. If an in-place operation modifies a tensor that is an input to an operation for which gradients need to be computed, PyTorch might raise an error or produce incorrect gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3065685"
      },
      "source": [
        "import torch\n",
        "\n",
        "print(\"--- Out-of-place Operations ---\")\n",
        "x = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
        "y = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
        "print(f\"Original x: {x}\")\n",
        "print(f\"Original y: {y}\")\n",
        "\n",
        "# Out-of-place addition\n",
        "z = x + y\n",
        "print(f\"Result of x + y (new tensor z): {z}\")\n",
        "print(f\"x after out-of-place operation: {x} (unchanged)\")\n",
        "\n",
        "# Out-of-place multiplication\n",
        "a = torch.tensor([10, 20, 30], dtype=torch.float32)\n",
        "b = a * 2\n",
        "print(f\"\\nOriginal a: {a}\")\n",
        "print(f\"Result of a * 2 (new tensor b): {b}\")\n",
        "print(f\"a after out-of-place operation: {a} (unchanged)\")\n",
        "\n",
        "print(\"\\n--- In-place Operations ---\")\n",
        "p = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
        "q = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
        "print(f\"Original p: {p}\")\n",
        "print(f\"Original q: {q}\")\n",
        "\n",
        "# In-place addition (p.add_(q) is equivalent to p += q)\n",
        "p.add_(q)\n",
        "print(f\"p after in-place add_ (p += q): {p} (modified)\")\n",
        "print(f\"q after in-place add_: {q} (unchanged)\")\n",
        "\n",
        "# Another in-place operation: multiplication with a scalar\n",
        "r = torch.tensor([10, 20, 30], dtype=torch.float32)\n",
        "print(f\"\\nOriginal r: {r}\")\n",
        "r.mul_(2)\n",
        "print(f\"r after in-place mul_ (r *= 2): {r} (modified)\")\n",
        "\n",
        "# In-place zeroing\n",
        "s = torch.tensor([100, 200, 300], dtype=torch.float32)\n",
        "print(f\"\\nOriginal s: {s}\")\n",
        "s.zero_()\n",
        "print(f\"s after in-place zero_(): {s} (modified)\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}