{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtcaDbe-p74j",
        "outputId": "8ab79c04-9945-4a3f-95fc-2120ddf3b3d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ko4cu8nWhtoQ3OoTqkJdesoq3GtiToev\n",
            "To: /content/poems-100.csv\n",
            "100%|██████████| 140k/140k [00:00<00:00, 38.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gdown\n",
        "\n",
        "\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1ko4cu8nWhtoQ3OoTqkJdesoq3GtiToev\"\n",
        "file_name = \"poems-100.csv\"\n",
        "\n",
        "if not os.path.exists(file_name):\n",
        "    gdown.download(url, file_name, quiet=False)\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "    txt = f.read().lower()\n",
        "\n",
        "words = txt.split()                     # Tokenize text\n",
        "vocab = sorted(set(words))              # Unique words\n",
        "stoi = {w:i for i,w in enumerate(vocab)}# Word → index\n",
        "itos = {i:w for w,i in stoi.items()}    # Index → word\n",
        "V = len(vocab)                          # Vocabulary size\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare sliding window sequences\n",
        "win = 5\n",
        "X_idx, y_idx = [], []\n",
        "\n",
        "for i in range(len(words) - win):\n",
        "    X_idx.append([stoi[w] for w in words[i:i+win]])\n",
        "    y_idx.append(stoi[words[i+win]])\n",
        "\n",
        "X_idx = torch.tensor(X_idx)\n",
        "y_idx = torch.tensor(y_idx)\n"
      ],
      "metadata": {
        "id": "XSovAZfOsYtq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert indices to one-hot vectors\n",
        "N, T = X_idx.shape\n",
        "X_hot = torch.zeros(N, T, V)\n",
        "\n",
        "for i in range(N):\n",
        "    for j in range(T):\n",
        "        X_hot[i, j, X_idx[i, j]] = 1.0\n",
        "\n",
        "hot_ds = TensorDataset(X_hot, y_idx)    # Dataset\n",
        "hot_dl = DataLoader(hot_ds, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "amY8HaMisZ8C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RNN using one-hot vectors\n",
        "class RNNHot(nn.Module):\n",
        "    def __init__(self, v, h):\n",
        "        super().__init__()\n",
        "        self.r = nn.RNN(v, h, batch_first=True)  # RNN layer\n",
        "        self.o = nn.Linear(h, v)                 # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        y, _ = self.r(x)\n",
        "        return self.o(y[:, -1])                  # Last timestep\n"
      ],
      "metadata": {
        "id": "hkSlU9G-shkB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train one-hot RNN\n",
        "net = RNNHot(V, 64)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "for e in range(20):\n",
        "    s = 0\n",
        "    for xb, yb in hot_dl:\n",
        "        p = net(xb)\n",
        "        l = loss_fn(p, yb)\n",
        "        opt.zero_grad()\n",
        "        l.backward()\n",
        "        opt.step()\n",
        "        s += l.item()\n",
        "    print(f\"Epoch {e+1}, Loss: {s/len(hot_dl):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af0JF9OKskvb",
        "outputId": "c8c408cb-5dde-4643-f9dc-e2d8bc1b30cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.4008\n",
            "Epoch 2, Loss: 6.7847\n",
            "Epoch 3, Loss: 6.4992\n",
            "Epoch 4, Loss: 6.2011\n",
            "Epoch 5, Loss: 5.9532\n",
            "Epoch 6, Loss: 5.7124\n",
            "Epoch 7, Loss: 5.4609\n",
            "Epoch 8, Loss: 5.1973\n",
            "Epoch 9, Loss: 4.9165\n",
            "Epoch 10, Loss: 4.6397\n",
            "Epoch 11, Loss: 4.3700\n",
            "Epoch 12, Loss: 4.1097\n",
            "Epoch 13, Loss: 3.8600\n",
            "Epoch 14, Loss: 3.6142\n",
            "Epoch 15, Loss: 3.3816\n",
            "Epoch 16, Loss: 3.1562\n",
            "Epoch 17, Loss: 2.9410\n",
            "Epoch 18, Loss: 2.7313\n",
            "Epoch 19, Loss: 2.5361\n",
            "Epoch 20, Loss: 2.3414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generation for one-hot models\n",
        "def gen_hot(m, seed, n):\n",
        "    m.eval()\n",
        "    out = list(seed)\n",
        "\n",
        "    for _ in range(n):\n",
        "        cur = out[-T:]\n",
        "        ids = [stoi[w] for w in cur]\n",
        "        x = torch.zeros(1, len(ids), V)\n",
        "        for i, k in enumerate(ids):\n",
        "            x[0, i, k] = 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            p = m(x)\n",
        "\n",
        "        nxt = torch.argmax(p).item()\n",
        "        out.append(itos[nxt])\n",
        "    return \" \".join(out)\n",
        "\n",
        "print(gen_hot(net, [\"the\",\"sun\",\"shines\",\"bright\",\"upon\"], 50))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBu1Y4wRsmUI",
        "outputId": "0128f67d-1d9d-42eb-ecda-07e5eac98134"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the sun shines bright upon warmer than the song of hate on a lovelier, or it is know like of dictionary, and often like you swings they dimmed, with the tune that fair lips with his lovers different earth and his of the centre, of his a new comer, by he at and strange, in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM version using one-hot vectors\n",
        "class LSTMHot(nn.Module):\n",
        "    def __init__(self, v, h):\n",
        "        super().__init__()\n",
        "        self.l = nn.LSTM(v, h, batch_first=True)  # LSTM layer\n",
        "        self.o = nn.Linear(h, v)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y, _ = self.l(x)\n",
        "        return self.o(y[:, -1])\n"
      ],
      "metadata": {
        "id": "smWYcee-spFr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train one-hot LSTM\n",
        "net = LSTMHot(V, 64)\n",
        "opt = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "for e in range(20):\n",
        "    s = 0\n",
        "    for xb, yb in hot_dl:\n",
        "        p = net(xb)\n",
        "        l = loss_fn(p, yb)\n",
        "        opt.zero_grad()\n",
        "        l.backward()\n",
        "        opt.step()\n",
        "        s += l.item()\n",
        "    print(f\"Epoch {e+1}, Loss: {s/len(hot_dl):.4f}\")\n",
        "\n",
        "print(gen_hot(net, [\"the\",\"sun\",\"shines\",\"bright\",\"upon\"], 50))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc85vXk0sqGo",
        "outputId": "72165eeb-ac78-4538-ce6a-a366833d7367"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.4592\n",
            "Epoch 2, Loss: 6.8660\n",
            "Epoch 3, Loss: 6.7024\n",
            "Epoch 4, Loss: 6.5127\n",
            "Epoch 5, Loss: 6.2933\n",
            "Epoch 6, Loss: 6.0509\n",
            "Epoch 7, Loss: 5.7852\n",
            "Epoch 8, Loss: 5.5068\n",
            "Epoch 9, Loss: 5.2276\n",
            "Epoch 10, Loss: 4.9481\n",
            "Epoch 11, Loss: 4.6729\n",
            "Epoch 12, Loss: 4.4015\n",
            "Epoch 13, Loss: 4.1324\n",
            "Epoch 14, Loss: 3.8695\n",
            "Epoch 15, Loss: 3.6120\n",
            "Epoch 16, Loss: 3.3611\n",
            "Epoch 17, Loss: 3.1167\n",
            "Epoch 18, Loss: 2.8738\n",
            "Epoch 19, Loss: 2.6433\n",
            "Epoch 20, Loss: 2.4156\n",
            "the sun shines bright upon more than the young little i am i know i am the spirit of one and all all the best of my and looking and i so that of the wild is like in these the dead of all and ever the wild of long sweet these an an or\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_ds = TensorDataset(X_idx, y_idx)\n",
        "emb_dl = DataLoader(emb_ds, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "_YktL8zMsrjW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN with word embeddings\n",
        "class RNNEmb(nn.Module):\n",
        "    def __init__(self, v, e, h):\n",
        "        super().__init__()\n",
        "        self.e = nn.Embedding(v, e)     # Embedding layer\n",
        "        self.r = nn.RNN(e, h, batch_first=True)\n",
        "        self.o = nn.Linear(h, v)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.e(x)\n",
        "        y, _ = self.r(x)\n",
        "        return self.o(y[:, -1])\n"
      ],
      "metadata": {
        "id": "I99iduOBsuem"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train embedding-based RNN\n",
        "net = RNNEmb(V, 100, 64)\n",
        "opt = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "for e in range(20):\n",
        "    s = 0\n",
        "    for xb, yb in emb_dl:\n",
        "        p = net(xb)\n",
        "        l = loss_fn(p, yb)\n",
        "        opt.zero_grad()\n",
        "        l.backward()\n",
        "        opt.step()\n",
        "        s += l.item()\n",
        "    print(f\"Epoch {e+1}, Loss: {s/len(emb_dl):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG8yoAnAswAx",
        "outputId": "2e2904dc-ca2c-462d-c9a6-3d4bab4aaa7a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.4817\n",
            "Epoch 2, Loss: 6.5976\n",
            "Epoch 3, Loss: 6.2512\n",
            "Epoch 4, Loss: 5.9349\n",
            "Epoch 5, Loss: 5.6291\n",
            "Epoch 6, Loss: 5.3326\n",
            "Epoch 7, Loss: 5.0444\n",
            "Epoch 8, Loss: 4.7630\n",
            "Epoch 9, Loss: 4.4880\n",
            "Epoch 10, Loss: 4.2212\n",
            "Epoch 11, Loss: 3.9675\n",
            "Epoch 12, Loss: 3.7225\n",
            "Epoch 13, Loss: 3.4913\n",
            "Epoch 14, Loss: 3.2776\n",
            "Epoch 15, Loss: 3.0768\n",
            "Epoch 16, Loss: 2.8901\n",
            "Epoch 17, Loss: 2.7171\n",
            "Epoch 18, Loss: 2.5502\n",
            "Epoch 19, Loss: 2.3956\n",
            "Epoch 20, Loss: 2.2476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generation for embedding models\n",
        "def gen_emb(m, seed, n):\n",
        "    m.eval()\n",
        "    out = list(seed)\n",
        "\n",
        "    for _ in range(n):\n",
        "        cur = out[-T:]\n",
        "        x = torch.tensor([stoi[w] for w in cur]).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            p = m(x)\n",
        "\n",
        "        nxt = torch.argmax(p).item()\n",
        "        out.append(itos[nxt])\n",
        "    return \" \".join(out)\n",
        "\n",
        "print(gen_emb(net, [\"the\",\"sun\",\"shines\",\"bright\",\"upon\"], 50))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6WNUIrKsx4p",
        "outputId": "f5f1d9d8-02bf-4e43-b9e0-84339dc1bb0d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the sun shines bright upon his own dashings—yet—the dead are there: and she \" i am not i see and i see the day with stealthy tread, leaving me baskets cover'd with their shimmering sound; and frogs in the sea! and i am the poet of the young men glisten'd with him who died and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM with embeddings\n",
        "class LSTMEmb(nn.Module):\n",
        "    def __init__(self, v, e, h):\n",
        "        super().__init__()\n",
        "        self.e = nn.Embedding(v, e)\n",
        "        self.l = nn.LSTM(e, h, batch_first=True)\n",
        "        self.o = nn.Linear(h, v)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.e(x)\n",
        "        y, _ = self.l(x)\n",
        "        return self.o(y[:, -1])\n"
      ],
      "metadata": {
        "id": "EM36-zMss0fv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train embedding-based LSTM\n",
        "net = LSTMEmb(V, 100, 64)\n",
        "opt = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "for e in range(20):\n",
        "    s = 0\n",
        "    for xb, yb in emb_dl:\n",
        "        p = net(xb)\n",
        "        l = loss_fn(p, yb)\n",
        "        opt.zero_grad()\n",
        "        l.backward()\n",
        "        opt.step()\n",
        "        s += l.item()\n",
        "    print(f\"Epoch {e+1}, Loss: {s/len(emb_dl):.4f}\")\n",
        "\n",
        "print(gen_emb(net, [\"the\",\"sun\",\"shines\",\"bright\",\"upon\"], 50))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-lXfTePs2bx",
        "outputId": "19fb3393-f124-412c-ad6c-a4d46c8e5d23"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.4668\n",
            "Epoch 2, Loss: 6.7015\n",
            "Epoch 3, Loss: 6.4220\n",
            "Epoch 4, Loss: 6.1223\n",
            "Epoch 5, Loss: 5.8242\n",
            "Epoch 6, Loss: 5.5295\n",
            "Epoch 7, Loss: 5.2410\n",
            "Epoch 8, Loss: 4.9570\n",
            "Epoch 9, Loss: 4.6777\n",
            "Epoch 10, Loss: 4.4010\n",
            "Epoch 11, Loss: 4.1299\n",
            "Epoch 12, Loss: 3.8663\n",
            "Epoch 13, Loss: 3.6106\n",
            "Epoch 14, Loss: 3.3639\n",
            "Epoch 15, Loss: 3.1286\n",
            "Epoch 16, Loss: 2.9043\n",
            "Epoch 17, Loss: 2.6935\n",
            "Epoch 18, Loss: 2.4949\n",
            "Epoch 19, Loss: 2.3105\n",
            "Epoch 20, Loss: 2.1373\n",
            "the sun shines bright upon me, and all the argument of the same or the moon where the squatter strikes of the tale of the murder of the beautiful annabel lee; and the same waits in the field who would not the young husband sleeps by his wife; and the narrowest hinge in my hand\n"
          ]
        }
      ]
    }
  ]
}